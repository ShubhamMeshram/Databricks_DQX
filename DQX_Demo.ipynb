{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "196a8401-2469-4901-b38e-0d9df0c28475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "pip install databricks-labs-dqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e1f8bf-5d10-4091-8ae6-185d9c09a319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls dbfs:/databricks-datasets/flights/departuredelays.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dabe36f-fcfd-4bc5-95e8-d8f8f9f35bc6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_97d3dc34\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_4240bd2e\",\"enabled\":true,\"columnId\":\"delay\",\"dataType\":\"integer\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1748639001717}],\"syncTimestamp\":1748639001718}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"dbfs:/databricks-datasets/flights/departuredelays.csv\", header=True, inferSchema=True)\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# df = df.withColumn(\"DelayInHours\", col(\"delay\") / 60)\n",
    "# df = df.withColumn(\"IsDelayed\", col(\"delay\") > 0)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503d3fee-bb2b-4e68-910e-75e5516f243e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook Source\n",
    "# MAGIC %md\n",
    "# MAGIC # Databricks DQX Showcase: Flight Departure Delays Data Quality\n",
    "# MAGIC\n",
    "# MAGIC This notebook demonstrates how to use Databricks DQX (Data Quality eXtensions) to define, run, and visualize data quality checks on the `departuredelays.csv` dataset.\n",
    "# MAGIC\n",
    "# MAGIC We'll focus on validating columns like `date`, `delay`, `distance`, `origin`, and `destination`.\n",
    "# MAGIC\n",
    "# MAGIC **DQX is an open-source project from Databricks Labs.** This means it's community-driven and does not come with official Databricks customer support like core products. Support is primarily via GitHub issues and community forums.\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Install Databricks DQX\n",
    "# MAGIC\n",
    "# MAGIC First, ensure DQX is installed in your cluster environment.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#pip install databricks-dqx\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Load the Dataset\n",
    "# MAGIC\n",
    "# MAGIC We'll load the default `dbfs:/databricks-datasets/flights/departuredelays.csv` dataset.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DQX_Demo\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(\"dbfs:/databricks-datasets/flights/departuredelays.csv\")\n",
    "\n",
    "# Cast 'date' to a proper date type for date-related checks\n",
    "df = df.withColumn(\"date\", to_date(col(\"date\").cast(\"string\"), \"MMddHHmm\"))\n",
    "\n",
    "# Create a temporary view for DQX to work with SQL expressions\n",
    "df.createOrReplaceTempView(\"departure_delays\")\n",
    "\n",
    "print(f\"Loaded DataFrame with {df.count()} rows and {len(df.columns)} columns.\")\n",
    "df.printSchema()\n",
    "df.display()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Initialize DQX API\n",
    "# MAGIC\n",
    "# MAGIC We need to create an instance of the `DataQualityAPI` to start defining and running checks.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from databricks.dqx.api.data_quality_api import DataQualityAPI\n",
    "from databricks.dqx.checks.check import Check, CheckType\n",
    "\n",
    "dq_api = DataQualityAPI()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Define Data Quality Checks\n",
    "# MAGIC\n",
    "# MAGIC Here, we'll define various data quality checks using DQX's declarative syntax. We'll cover common checks for our flight delays dataset columns.\n",
    "# MAGIC\n",
    "# MAGIC **Columns:** `date`, `delay`, `distance`, `origin`, `destination`\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define a list to hold our data quality checks\n",
    "checks = []\n",
    "\n",
    "# --- General Checks ---\n",
    "# Check for duplicate rows in the entire dataset\n",
    "checks.append(Check.no_duplicate_rows().set_check_name(\"NoDuplicateFlights\"))\n",
    "\n",
    "# --- 'date' column checks ---\n",
    "# Ensure 'date' is not null\n",
    "checks.append(Check.not_null(\"date\").set_check_name(\"DateNotNull\"))\n",
    "# Ensure 'date' falls within a reasonable historical range (e.g., 2000-2025)\n",
    "checks.append(Check.column_values_between(\"date\", \"2000-01-01\", \"2025-12-31\",\n",
    "                                          is_expression=True).set_check_name(\"DateWithinRange\"))\n",
    "\n",
    "# --- 'delay' column checks ---\n",
    "# Ensure 'delay' is not null\n",
    "checks.append(Check.not_null(\"delay\").set_check_name(\"DelayNotNull\"))\n",
    "# Ensure 'delay' is non-negative (delays can't be negative in this context)\n",
    "checks.append(Check.column_values_greater_than_or_equal_to(\"delay\", 0).set_check_name(\"DelayNonNegative\"))\n",
    "# Ensure 'delay' is within a realistic range (e.g., max 1440 minutes = 24 hours)\n",
    "checks.append(Check.column_values_between(\"delay\", 0, 1440).set_check_name(\"DelayRealisticRange\"))\n",
    "\n",
    "\n",
    "# --- 'distance' column checks ---\n",
    "# Ensure 'distance' is not null\n",
    "checks.append(Check.not_null(\"distance\").set_check_name(\"DistanceNotNull\"))\n",
    "# Ensure 'distance' is positive (a flight must cover a distance)\n",
    "checks.append(Check.column_values_greater_than(\"distance\", 0).set_check_name(\"DistancePositive\"))\n",
    "\n",
    "\n",
    "# --- 'origin' and 'destination' column checks ---\n",
    "# Ensure 'origin' is not null\n",
    "checks.append(Check.not_null(\"origin\").set_check_name(\"OriginNotNull\"))\n",
    "# Ensure 'destination' is not null\n",
    "checks.append(Check.not_null(\"destination\").set_check_name(\"DestinationNotNull\"))\n",
    "\n",
    "# Example: Check if origin/destination values are from a known set of major airports (illustrative)\n",
    "# For a real scenario, you'd load this from a lookup table.\n",
    "major_airports = [\"SFO\", \"LAX\", \"ORD\", \"JFK\", \"ATL\", \"DFW\", \"DEN\", \"CLT\", \"SEA\", \"LAS\"]\n",
    "checks.append(Check.column_values_in(\"origin\", major_airports).set_check_name(\"OriginInMajorAirports\"))\n",
    "checks.append(Check.column_values_in(\"destination\", major_airports).set_check_name(\"DestinationInMajorAirports\"))\n",
    "\n",
    "\n",
    "print(f\"Defined {len(checks)} data quality checks.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Run Data Quality Checks\n",
    "# MAGIC\n",
    "# MAGIC Now, we'll execute the defined checks against our `departure_delays` table.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Run the checks on the temporary view 'departure_delays'\n",
    "results = dq_api.run_quality_checks(\n",
    "    table_name=\"departure_delays\",\n",
    "    checks=checks\n",
    ")\n",
    "\n",
    "print(\"Data Quality Checks Executed.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Display Results Summary\n",
    "# MAGIC\n",
    "# MAGIC You can inspect the `results` object to see the outcome of each check.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display summary results\n",
    "results_df = results.get_results_df()\n",
    "results_df.display()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Generate and Display DQX Dashboard\n",
    "# MAGIC\n",
    "# MAGIC One of the most powerful features of DQX is its ability to generate an interactive HTML dashboard. This dashboard provides a visual summary of your data quality results, making it easy to identify issues.\n",
    "# MAGIC\n",
    "# MAGIC The dashboard will be saved to a DBFS path, which you can then download to view in your browser.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import os\n",
    "\n",
    "# Define a path where the dashboard HTML file will be saved.\n",
    "# Using a temporary directory within DBFS for simplicity.\n",
    "dashboard_output_path = \"/dbfs/tmp/dqx_dashboard/flight_delays_dashboard.html\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(dashboard_output_path), exist_ok=True)\n",
    "\n",
    "# Generate the dashboard\n",
    "dashboard_html_path = dq_api.generate_dashboard(\n",
    "    quality_check_results=results,\n",
    "    output_path=dashboard_output_path,\n",
    "    output_format=\"html\"\n",
    ")\n",
    "\n",
    "print(f\"DQX Dashboard generated at: {dashboard_html_path}\")\n",
    "print(f\"To view the dashboard, you can download it from this DBFS path using Databricks UI (Data -> DBFS browser) or Databricks CLI:\")\n",
    "print(f\"  dbfs:/tmp/dqx_dashboard/flight_delays_dashboard.html\")\n",
    "\n",
    "# You might be able to display it directly in the notebook if the environment supports it,\n",
    "# but downloading and opening in a browser provides the best interactive experience.\n",
    "# Example of displaying partial content for demonstration (might not be fully interactive in all notebooks)\n",
    "# with open(dashboard_output_path, 'r') as f:\n",
    "#   html_content = f.read()\n",
    "# displayHTML(html_content)\n",
    "\n",
    "# COMMAND ----------"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7478356647026157,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DQX_Demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
