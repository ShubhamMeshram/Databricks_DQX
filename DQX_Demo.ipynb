{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920fc722-8484-4e46-8a2a-df321403ca4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.generator import DQGenerator\n",
    "from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import yaml\n",
    "from databricks.labs.dqx.contexts.workspace import WorkspaceContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26cdd117-84c2-445c-8f07-c1a0e5a58e6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the WorkspaceClient to interact with the Databricks workspace\n",
    "ws = WorkspaceClient()\n",
    "\n",
    "# Initialize a DQProfiler instance with the workspace client\n",
    "profiler = DQProfiler(ws)\n",
    "\n",
    "generator = DQGenerator(ws)\n",
    "\n",
    "dq_engine = DQEngine(ws)\n",
    "\n",
    "\n",
    "ws = WorkspaceClient()\n",
    "profiler = DQProfiler(ws)\n",
    "generator = DQGenerator(ws)\n",
    "dlt_generator = DQDltGenerator(ws)\n",
    "dq_engine = DQEngine(ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24f048fa-a0c6-4c77-859c-5b3eb06a2e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open( \"custom_dqx_rules.yaml\", \"r\") as file:\n",
    "    check_dict = yaml.safe_load(file)\n",
    "\n",
    "dq_engine = DQEngine(spark)\n",
    "validation_result = dq_engine.validate_checks(check_dict)\n",
    "\n",
    "assert not validation_result.has_errors, f\"Validation failed: {validation_result.errors}\"\n",
    "\n",
    "silver_df, quarantine_df = dq_engine.apply_checks_by_metadata_and_split(input_df, check_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a69e623c-c835-46e1-b916-d7763d095748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(quarantine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "232914ba-2398-4372-9ec5-f3a669c42994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a DQEngine instance with the WorkspaceClient\n",
    "dq_engine = DQEngine(WorkspaceClient())\n",
    "\n",
    "# Apply quality checks and split the DataFrame into silver and quarantine DataFrames\n",
    "silver_df, quarantine_df = dq_engine.apply_checks_by_metadata_and_split(input_df, check_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7064ea1d-22c3-4e3c-b881-7e3613d4bf2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(quarantine_df.count())\n",
    "display(silver_df.count())\n",
    "display(input_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9005e779-28bd-44fc-aef4-a37d11192c60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ctx = WorkspaceContext(WorkspaceClient())\n",
    "dashboards_folder_link = f\"{ctx.installation.workspace_link('https://adb-8333330282859393.13.azuredatabricks.net/')}dashboards/\"\n",
    "print(f\"Open a dashboard from the following folder and refresh it:\")\n",
    "print(dashboards_folder_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835ba207-20aa-42a3-a153-f24a07e3ee1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the WorkspaceClient to interact with the Databricks workspace\n",
    "ws = WorkspaceClient()\n",
    "\n",
    "# Initialize a DQProfiler instance with the workspace client\n",
    "profiler = DQProfiler(ws)\n",
    "\n",
    "# Read the input data from a Delta table\n",
    "input_df = spark.read.csv(\"dbfs:/databricks-datasets/flights/departuredelays.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Display a sample of the input data\n",
    "input_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06cd5a1-2452-4aa7-8fcb-347314fe7866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_df = spark.read.csv(\"dbfs:/databricks-datasets/flights/departuredelays.csv\", header=True, inferSchema=True)\n",
    "summary_stats, profiles = profiler.profile(input_df, opts={\"sample_fraction\": 1.0})\n",
    "print(yaml.safe_dump(summary_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74b4daf-1920-4ff0-bfa0-b2f633dda517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for profile in profiles:\n",
    "    print('*',profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12792f47-5608-426d-9b08-417b27207ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generator = DQGenerator(ws)\n",
    "checks = generator.generate_dq_rules(profiles)  # with default level \"error\"\n",
    "print(yaml.safe_dump(checks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50cd20bd-9345-4a76-9b11-5afc33124626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt_generator = DQDltGenerator(ws)\n",
    "\n",
    "dlt_expectations = dlt_generator.generate_dlt_rules(profiles, language=\"SQL\")\n",
    "print(dlt_expectations)\n",
    "\n",
    "dlt_expectations = dlt_generator.generate_dlt_rules(profiles, language=\"Python\")\n",
    "print(dlt_expectations)\n",
    "\n",
    "dlt_expectations = dlt_generator.generate_dlt_rules(profiles, language=\"Python_Dict\")\n",
    "print(dlt_expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02f28289-7a1a-4b9d-a05d-6571a8a89cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_name = spark.sql(\"select current_user() as user\").collect()[0][\"user\"]\n",
    "checks_file = f\"/Workspace/Users/{user_name}/dqx_demo_checks.yml\"\n",
    "dq_engine = DQEngine(ws)\n",
    "dq_engine.save_checks_in_workspace_file(checks=checks, workspace_path=checks_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d56f5b87-2258-4fbe-bf7c-66ea7dafda21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dq_engine.save_checks_in_table(checks=checks, table_name=\"main.default.dqx_checks_table\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503d3fee-bb2b-4e68-910e-75e5516f243e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook Source\n",
    "# MAGIC %md\n",
    "# MAGIC # Databricks DQX Showcase: Flight Departure Delays Data Quality\n",
    "# MAGIC\n",
    "# MAGIC This notebook demonstrates how to use Databricks DQX (Data Quality eXtensions) to define, run, and visualize data quality checks on the `departuredelays.csv` dataset.\n",
    "# MAGIC\n",
    "# MAGIC We'll focus on validating columns like `date`, `delay`, `distance`, `origin`, and `destination`.\n",
    "# MAGIC\n",
    "# MAGIC **DQX is an open-source project from Databricks Labs.** This means it's community-driven and does not come with official Databricks customer support like core products. Support is primarily via GitHub issues and community forums.\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Install Databricks DQX\n",
    "# MAGIC\n",
    "# MAGIC First, ensure DQX is installed in your cluster environment.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#pip install databricks-dqx\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Load the Dataset\n",
    "# MAGIC\n",
    "# MAGIC We'll load the default `dbfs:/databricks-datasets/flights/departuredelays.csv` dataset.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DQX_Demo\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(\"dbfs:/databricks-datasets/flights/departuredelays.csv\")\n",
    "\n",
    "# Cast 'date' to a proper date type for date-related checks\n",
    "df = df.withColumn(\"date\", to_date(col(\"date\").cast(\"string\"), \"MMddHHmm\"))\n",
    "\n",
    "# Create a temporary view for DQX to work with SQL expressions\n",
    "df.createOrReplaceTempView(\"departure_delays\")\n",
    "\n",
    "print(f\"Loaded DataFrame with {df.count()} rows and {len(df.columns)} columns.\")\n",
    "df.printSchema()\n",
    "df.display()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Initialize DQX API\n",
    "# MAGIC\n",
    "# MAGIC We need to create an instance of the `DataQualityAPI` to start defining and running checks.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from databricks.dqx.api.data_quality_api import DataQualityAPI\n",
    "from databricks.dqx.checks.check import Check, CheckType\n",
    "\n",
    "dq_api = DataQualityAPI()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Define Data Quality Checks\n",
    "# MAGIC\n",
    "# MAGIC Here, we'll define various data quality checks using DQX's declarative syntax. We'll cover common checks for our flight delays dataset columns.\n",
    "# MAGIC\n",
    "# MAGIC **Columns:** `date`, `delay`, `distance`, `origin`, `destination`\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define a list to hold our data quality checks\n",
    "checks = []\n",
    "\n",
    "# --- General Checks ---\n",
    "# Check for duplicate rows in the entire dataset\n",
    "checks.append(Check.no_duplicate_rows().set_check_name(\"NoDuplicateFlights\"))\n",
    "\n",
    "# --- 'date' column checks ---\n",
    "# Ensure 'date' is not null\n",
    "checks.append(Check.not_null(\"date\").set_check_name(\"DateNotNull\"))\n",
    "# Ensure 'date' falls within a reasonable historical range (e.g., 2000-2025)\n",
    "checks.append(Check.column_values_between(\"date\", \"2000-01-01\", \"2025-12-31\",\n",
    "                                          is_expression=True).set_check_name(\"DateWithinRange\"))\n",
    "\n",
    "# --- 'delay' column checks ---\n",
    "# Ensure 'delay' is not null\n",
    "checks.append(Check.not_null(\"delay\").set_check_name(\"DelayNotNull\"))\n",
    "# Ensure 'delay' is non-negative (delays can't be negative in this context)\n",
    "checks.append(Check.column_values_greater_than_or_equal_to(\"delay\", 0).set_check_name(\"DelayNonNegative\"))\n",
    "# Ensure 'delay' is within a realistic range (e.g., max 1440 minutes = 24 hours)\n",
    "checks.append(Check.column_values_between(\"delay\", 0, 1440).set_check_name(\"DelayRealisticRange\"))\n",
    "\n",
    "\n",
    "# --- 'distance' column checks ---\n",
    "# Ensure 'distance' is not null\n",
    "checks.append(Check.not_null(\"distance\").set_check_name(\"DistanceNotNull\"))\n",
    "# Ensure 'distance' is positive (a flight must cover a distance)\n",
    "checks.append(Check.column_values_greater_than(\"distance\", 0).set_check_name(\"DistancePositive\"))\n",
    "\n",
    "\n",
    "# --- 'origin' and 'destination' column checks ---\n",
    "# Ensure 'origin' is not null\n",
    "checks.append(Check.not_null(\"origin\").set_check_name(\"OriginNotNull\"))\n",
    "# Ensure 'destination' is not null\n",
    "checks.append(Check.not_null(\"destination\").set_check_name(\"DestinationNotNull\"))\n",
    "\n",
    "# Example: Check if origin/destination values are from a known set of major airports (illustrative)\n",
    "# For a real scenario, you'd load this from a lookup table.\n",
    "major_airports = [\"SFO\", \"LAX\", \"ORD\", \"JFK\", \"ATL\", \"DFW\", \"DEN\", \"CLT\", \"SEA\", \"LAS\"]\n",
    "checks.append(Check.column_values_in(\"origin\", major_airports).set_check_name(\"OriginInMajorAirports\"))\n",
    "checks.append(Check.column_values_in(\"destination\", major_airports).set_check_name(\"DestinationInMajorAirports\"))\n",
    "\n",
    "\n",
    "print(f\"Defined {len(checks)} data quality checks.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Run Data Quality Checks\n",
    "# MAGIC\n",
    "# MAGIC Now, we'll execute the defined checks against our `departure_delays` table.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Run the checks on the temporary view 'departure_delays'\n",
    "results = dq_api.run_quality_checks(\n",
    "    table_name=\"departure_delays\",\n",
    "    checks=checks\n",
    ")\n",
    "\n",
    "print(\"Data Quality Checks Executed.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Display Results Summary\n",
    "# MAGIC\n",
    "# MAGIC You can inspect the `results` object to see the outcome of each check.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display summary results\n",
    "results_df = results.get_results_df()\n",
    "results_df.display()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Generate and Display DQX Dashboard\n",
    "# MAGIC\n",
    "# MAGIC One of the most powerful features of DQX is its ability to generate an interactive HTML dashboard. This dashboard provides a visual summary of your data quality results, making it easy to identify issues.\n",
    "# MAGIC\n",
    "# MAGIC The dashboard will be saved to a DBFS path, which you can then download to view in your browser.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import os\n",
    "\n",
    "# Define a path where the dashboard HTML file will be saved.\n",
    "# Using a temporary directory within DBFS for simplicity.\n",
    "dashboard_output_path = \"/dbfs/tmp/dqx_dashboard/flight_delays_dashboard.html\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(dashboard_output_path), exist_ok=True)\n",
    "\n",
    "# Generate the dashboard\n",
    "dashboard_html_path = dq_api.generate_dashboard(\n",
    "    quality_check_results=results,\n",
    "    output_path=dashboard_output_path,\n",
    "    output_format=\"html\"\n",
    ")\n",
    "\n",
    "print(f\"DQX Dashboard generated at: {dashboard_html_path}\")\n",
    "print(f\"To view the dashboard, you can download it from this DBFS path using Databricks UI (Data -> DBFS browser) or Databricks CLI:\")\n",
    "print(f\"  dbfs:/tmp/dqx_dashboard/flight_delays_dashboard.html\")\n",
    "\n",
    "# You might be able to display it directly in the notebook if the environment supports it,\n",
    "# but downloading and opening in a browser provides the best interactive experience.\n",
    "# Example of displaying partial content for demonstration (might not be fully interactive in all notebooks)\n",
    "# with open(dashboard_output_path, 'r') as f:\n",
    "#   html_content = f.read()\n",
    "# displayHTML(html_content)\n",
    "\n",
    "# COMMAND ----------"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7478356647026157,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DQX_Demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
